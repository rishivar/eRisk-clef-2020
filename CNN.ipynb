{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, MaxPooling1D, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['tokens'] = data['TEXT'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "for l in data.truth:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        \n",
    "data['Pos']= pos\n",
    "data['Neg']= neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>tokens</th>\n",
       "      <th>truth</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject671</td>\n",
       "      <td>2018-05-03 04:56:17</td>\n",
       "      <td>there pizza place couple door couple kid worki...</td>\n",
       "      <td>[there, pizza, place, couple, door, couple, ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject671</td>\n",
       "      <td>2018-05-03 05:06:23</td>\n",
       "      <td>pour hot water pitcher americano shot pouring ...</td>\n",
       "      <td>[pour, hot, water, pitcher, americano, shot, p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject671</td>\n",
       "      <td>2018-05-03 06:04:10</td>\n",
       "      <td>ombre pink drink cool lime base think lot cust...</td>\n",
       "      <td>[ombre, pink, drink, cool, lime, base, think, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject671</td>\n",
       "      <td>2018-05-03 07:08:17</td>\n",
       "      <td>tall americano double sleeve cup</td>\n",
       "      <td>[tall, americano, double, sleeve, cup]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject671</td>\n",
       "      <td>2018-05-10 04:16:03</td>\n",
       "      <td>print blank receipt paper write closing list c...</td>\n",
       "      <td>[print, blank, receipt, paper, write, closing,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                 DATE  \\\n",
       "0  subject671  2018-05-03 04:56:17   \n",
       "1  subject671  2018-05-03 05:06:23   \n",
       "2  subject671  2018-05-03 06:04:10   \n",
       "3  subject671  2018-05-03 07:08:17   \n",
       "4  subject671  2018-05-10 04:16:03   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  there pizza place couple door couple kid worki...   \n",
       "1  pour hot water pitcher americano shot pouring ...   \n",
       "2  ombre pink drink cool lime base think lot cust...   \n",
       "3                   tall americano double sleeve cup   \n",
       "4  print blank receipt paper write closing list c...   \n",
       "\n",
       "                                              tokens  truth  Pos  Neg  \n",
       "0  [there, pizza, place, couple, door, couple, ki...      1    1    0  \n",
       "1  [pour, hot, water, pitcher, americano, shot, p...      1    1    0  \n",
       "2  [ombre, pink, drink, cool, lime, base, think, ...      1    1    0  \n",
       "3             [tall, americano, double, sleeve, cup]      1    1    0  \n",
       "4  [print, blank, receipt, paper, write, closing,...      1    1    0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['ID', 'DATE', 'TEXT', 'tokens', 'truth', 'Pos', 'Neg']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, \n",
    "                                         test_size=0.10, \n",
    "                                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1885388 words total, with a vocabulary size of 105829\n",
      "Max sentence length is 5768\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207510 words total, with a vocabulary size of 27817\n",
      "Max sentence length is 1622\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105829 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"TEXT\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"TEXT\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105830, 300)\n"
     ]
    }
   ],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"TEXT\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',f1_m,precision_m, recall_m])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Pos', 'Neg']\n",
    "y_train = data_train[label_names].values\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 100, 300)     31749000    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 99, 200)      120200      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 98, 200)      180200      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 97, 200)      240200      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 96, 200)      300200      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 95, 200)      360200      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 200)          0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 200)          0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 200)          0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 200)          0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 200)          0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1000)         0           global_max_pooling1d_26[0][0]    \n",
      "                                                                 global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "                                                                 global_max_pooling1d_30[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 1000)         0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 128)          128128      dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 128)          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            258         dropout_18[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 33,078,386\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 31,749,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102238 samples, validate on 11360 samples\n",
      "Epoch 1/10\n",
      "102238/102238 [==============================] - 31s 307us/step - loss: 0.2023 - acc: 0.9478 - f1_m: 0.9478 - precision_m: 0.9478 - recall_m: 0.9478 - val_loss: 0.1985 - val_acc: 0.9436 - val_f1_m: 0.9436 - val_precision_m: 0.9436 - val_recall_m: 0.9436\n",
      "Epoch 2/10\n",
      "102238/102238 [==============================] - 31s 305us/step - loss: 0.1748 - acc: 0.9489 - f1_m: 0.9489 - precision_m: 0.9489 - recall_m: 0.9489 - val_loss: 0.1978 - val_acc: 0.9466 - val_f1_m: 0.9466 - val_precision_m: 0.9466 - val_recall_m: 0.9466\n",
      "Epoch 3/10\n",
      "102238/102238 [==============================] - 31s 303us/step - loss: 0.1356 - acc: 0.9568 - f1_m: 0.9568 - precision_m: 0.9568 - recall_m: 0.9568 - val_loss: 0.2225 - val_acc: 0.9460 - val_f1_m: 0.9460 - val_precision_m: 0.9460 - val_recall_m: 0.9460\n",
      "Epoch 4/10\n",
      "102238/102238 [==============================] - 31s 306us/step - loss: 0.0944 - acc: 0.9675 - f1_m: 0.9675 - precision_m: 0.9675 - recall_m: 0.9675 - val_loss: 0.2824 - val_acc: 0.9382 - val_f1_m: 0.9382 - val_precision_m: 0.9381 - val_recall_m: 0.9383\n",
      "Epoch 5/10\n",
      "102238/102238 [==============================] - 32s 311us/step - loss: 0.0730 - acc: 0.9774 - f1_m: 0.9774 - precision_m: 0.9773 - recall_m: 0.9774 - val_loss: 0.3571 - val_acc: 0.9445 - val_f1_m: 0.9445 - val_precision_m: 0.9445 - val_recall_m: 0.9445\n",
      "Epoch 6/10\n",
      "102238/102238 [==============================] - 32s 312us/step - loss: 0.0626 - acc: 0.9807 - f1_m: 0.9807 - precision_m: 0.9807 - recall_m: 0.9807 - val_loss: 0.3931 - val_acc: 0.9383 - val_f1_m: 0.9383 - val_precision_m: 0.9383 - val_recall_m: 0.9383\n",
      "Epoch 7/10\n",
      "102238/102238 [==============================] - 32s 308us/step - loss: 0.0568 - acc: 0.9823 - f1_m: 0.9823 - precision_m: 0.9823 - recall_m: 0.9823 - val_loss: 0.3459 - val_acc: 0.9298 - val_f1_m: 0.9298 - val_precision_m: 0.9300 - val_recall_m: 0.9297\n",
      "Epoch 8/10\n",
      "102238/102238 [==============================] - 32s 313us/step - loss: 0.0506 - acc: 0.9846 - f1_m: 0.9846 - precision_m: 0.9845 - recall_m: 0.9846 - val_loss: 0.4808 - val_acc: 0.9422 - val_f1_m: 0.9422 - val_precision_m: 0.9422 - val_recall_m: 0.9423\n",
      "Epoch 9/10\n",
      "102238/102238 [==============================] - 31s 307us/step - loss: 0.0453 - acc: 0.9861 - f1_m: 0.9861 - precision_m: 0.9861 - recall_m: 0.9861 - val_loss: 0.4999 - val_acc: 0.9414 - val_f1_m: 0.9414 - val_precision_m: 0.9414 - val_recall_m: 0.9414\n",
      "Epoch 10/10\n",
      "102238/102238 [==============================] - 32s 314us/step - loss: 0.0425 - acc: 0.9871 - f1_m: 0.9871 - precision_m: 0.9870 - recall_m: 0.9871 - val_loss: 0.6025 - val_acc: 0.9359 - val_f1_m: 0.9359 - val_precision_m: 0.9359 - val_recall_m: 0.9359\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12623/12623 [==============================] - 1s 100us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "y_pred_bool = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(classification_report(data_test['truth'], y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102238 samples, validate on 11360 samples\n",
      "Epoch 1/10\n",
      "102238/102238 [==============================] - 31s 302us/step - loss: 0.0406 - acc: 0.9873 - f1_m: 0.9873 - precision_m: 0.9873 - recall_m: 0.9873 - val_loss: 0.4945 - val_acc: 0.9361 - val_f1_m: 0.9361 - val_precision_m: 0.9364 - val_recall_m: 0.9358\n",
      "Epoch 2/10\n",
      "102238/102238 [==============================] - 31s 301us/step - loss: 0.0338 - acc: 0.9893 - f1_m: 0.9893 - precision_m: 0.9893 - recall_m: 0.9893 - val_loss: 0.6525 - val_acc: 0.9410 - val_f1_m: 0.9410 - val_precision_m: 0.9409 - val_recall_m: 0.9410\n",
      "Epoch 3/10\n",
      "102238/102238 [==============================] - 31s 304us/step - loss: 0.0334 - acc: 0.9895 - f1_m: 0.9895 - precision_m: 0.9895 - recall_m: 0.9895 - val_loss: 0.5629 - val_acc: 0.9376 - val_f1_m: 0.9376 - val_precision_m: 0.9377 - val_recall_m: 0.9376\n",
      "Epoch 4/10\n",
      "102238/102238 [==============================] - 31s 303us/step - loss: 0.0317 - acc: 0.9901 - f1_m: 0.9901 - precision_m: 0.9901 - recall_m: 0.9901 - val_loss: 0.6089 - val_acc: 0.9412 - val_f1_m: 0.9412 - val_precision_m: 0.9412 - val_recall_m: 0.9412\n",
      "Epoch 5/10\n",
      "102238/102238 [==============================] - 31s 304us/step - loss: 0.0298 - acc: 0.9906 - f1_m: 0.9906 - precision_m: 0.9906 - recall_m: 0.9906 - val_loss: 0.6961 - val_acc: 0.9424 - val_f1_m: 0.9424 - val_precision_m: 0.9424 - val_recall_m: 0.9424\n",
      "Epoch 6/10\n",
      "102238/102238 [==============================] - 31s 307us/step - loss: 0.0279 - acc: 0.9912 - f1_m: 0.9912 - precision_m: 0.9912 - recall_m: 0.9912 - val_loss: 0.7516 - val_acc: 0.9423 - val_f1_m: 0.9423 - val_precision_m: 0.9423 - val_recall_m: 0.9423\n",
      "Epoch 7/10\n",
      "102238/102238 [==============================] - 31s 307us/step - loss: 0.0275 - acc: 0.9913 - f1_m: 0.9913 - precision_m: 0.9912 - recall_m: 0.9913 - val_loss: 0.6997 - val_acc: 0.9446 - val_f1_m: 0.9446 - val_precision_m: 0.9446 - val_recall_m: 0.9446\n",
      "Epoch 8/10\n",
      "102238/102238 [==============================] - 32s 310us/step - loss: 0.0251 - acc: 0.9922 - f1_m: 0.9922 - precision_m: 0.9923 - recall_m: 0.9922 - val_loss: 0.7450 - val_acc: 0.9393 - val_f1_m: 0.9394 - val_precision_m: 0.9393 - val_recall_m: 0.9394\n",
      "Epoch 9/10\n",
      "102238/102238 [==============================] - 32s 308us/step - loss: 0.0251 - acc: 0.9917 - f1_m: 0.9917 - precision_m: 0.9917 - recall_m: 0.9917 - val_loss: 0.7098 - val_acc: 0.9386 - val_f1_m: 0.9386 - val_precision_m: 0.9386 - val_recall_m: 0.9386\n",
      "Epoch 10/10\n",
      "102238/102238 [==============================] - 31s 307us/step - loss: 0.0248 - acc: 0.9920 - f1_m: 0.9920 - precision_m: 0.9920 - recall_m: 0.9920 - val_loss: 0.6448 - val_acc: 0.9368 - val_f1_m: 0.9368 - val_precision_m: 0.9375 - val_recall_m: 0.9361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f72d04291d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(10589, 100, input_length=50))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "model_conv = create_conv_model()\n",
    "model.fit(x_train, y_tr, validation_split=0.1, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12623/12623 [==============================] - 1s 62us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.02      0.04     11938\n",
      "           1       0.05      0.81      0.09       685\n",
      "\n",
      "    accuracy                           0.06     12623\n",
      "   macro avg       0.36      0.42      0.06     12623\n",
      "weighted avg       0.64      0.06      0.04     12623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "y_pred_bool = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(classification_report(data_test['truth'], y_pred_bool))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow1.1] *",
   "language": "python",
   "name": "conda-env-tensorflow1.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
