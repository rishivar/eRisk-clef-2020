{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, LSTM, GRU, Dropout, Reshape, Flatten, concatenate, Input, MaxPooling1D, Conv1D,GlobalAveragePooling1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['tokens'] = data['TEXT'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "for l in data.truth:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        \n",
    "data['Pos']= pos\n",
    "data['Neg']= neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>tokens</th>\n",
       "      <th>truth</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there pizza place couple door couple kid worki...</td>\n",
       "      <td>[there, pizza, place, couple, door, couple, ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pour hot water pitcher americano shot pouring ...</td>\n",
       "      <td>[pour, hot, water, pitcher, americano, shot, p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ombre pink drink cool lime base think lot cust...</td>\n",
       "      <td>[ombre, pink, drink, cool, lime, base, think, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tall americano double sleeve cup</td>\n",
       "      <td>[tall, americano, double, sleeve, cup]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>print blank receipt paper write closing list c...</td>\n",
       "      <td>[print, blank, receipt, paper, write, closing,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  \\\n",
       "0  there pizza place couple door couple kid worki...   \n",
       "1  pour hot water pitcher americano shot pouring ...   \n",
       "2  ombre pink drink cool lime base think lot cust...   \n",
       "3                   tall americano double sleeve cup   \n",
       "4  print blank receipt paper write closing list c...   \n",
       "\n",
       "                                              tokens  truth  Pos  Neg  \n",
       "0  [there, pizza, place, couple, door, couple, ki...      1    1    0  \n",
       "1  [pour, hot, water, pitcher, americano, shot, p...      1    1    0  \n",
       "2  [ombre, pink, drink, cool, lime, base, think, ...      1    1    0  \n",
       "3             [tall, americano, double, sleeve, cup]      1    1    0  \n",
       "4  [print, blank, receipt, paper, write, closing,...      1    1    0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['TEXT', 'tokens', 'truth', 'Pos', 'Neg']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, \n",
    "                                         test_size=0.10, \n",
    "                                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1885388 words total, with a vocabulary size of 105829\n",
      "Max sentence length is 5768\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207510 words total, with a vocabulary size of 27817\n",
      "Max sentence length is 1622\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences = data['tokens'], size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "filename = 'gensim_word2vec_model.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105829 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"TEXT\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"TEXT\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105830, 300)\n"
     ]
    }
   ],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"TEXT\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Pos', 'Neg']\n",
    "y_train = data_train[label_names].values\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     31749000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 199, 200)     120200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 198, 200)     180200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 197, 200)     240200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 196, 200)     300200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 195, 200)     360200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000)         0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          128128      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 33,078,386\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 31,749,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102238 samples, validate on 11360 samples\n",
      "Epoch 1/50\n",
      "102238/102238 [==============================] - 61s 601us/step - loss: 0.2022 - acc: 0.9478 - val_loss: 0.2031 - val_acc: 0.9436\n",
      "Epoch 2/50\n",
      "102238/102238 [==============================] - 51s 501us/step - loss: 0.1772 - acc: 0.9480 - val_loss: 0.2016 - val_acc: 0.9436\n",
      "Epoch 3/50\n",
      "102238/102238 [==============================] - 51s 502us/step - loss: 0.1408 - acc: 0.9551 - val_loss: 0.2211 - val_acc: 0.9445\n",
      "Epoch 4/50\n",
      "102238/102238 [==============================] - 51s 503us/step - loss: 0.0962 - acc: 0.9694 - val_loss: 0.2494 - val_acc: 0.9411\n",
      "Epoch 5/50\n",
      "102238/102238 [==============================] - 51s 501us/step - loss: 0.0729 - acc: 0.9767 - val_loss: 0.3586 - val_acc: 0.9438\n",
      "Epoch 6/50\n",
      "102238/102238 [==============================] - 51s 499us/step - loss: 0.0625 - acc: 0.9807 - val_loss: 0.3478 - val_acc: 0.9382\n",
      "Epoch 7/50\n",
      "102238/102238 [==============================] - 51s 500us/step - loss: 0.0551 - acc: 0.9829 - val_loss: 0.3490 - val_acc: 0.9397\n",
      "Epoch 8/50\n",
      "102238/102238 [==============================] - 52s 505us/step - loss: 0.0489 - acc: 0.9845 - val_loss: 0.5202 - val_acc: 0.9446\n",
      "Epoch 9/50\n",
      "102238/102238 [==============================] - 51s 500us/step - loss: 0.0463 - acc: 0.9858 - val_loss: 0.4258 - val_acc: 0.9437\n",
      "Epoch 10/50\n",
      "102238/102238 [==============================] - 51s 500us/step - loss: 0.0416 - acc: 0.9873 - val_loss: 0.6331 - val_acc: 0.9440\n",
      "Epoch 11/50\n",
      "102238/102238 [==============================] - 52s 505us/step - loss: 0.0399 - acc: 0.9873 - val_loss: 0.4514 - val_acc: 0.9414\n",
      "Epoch 12/50\n",
      "102238/102238 [==============================] - 51s 500us/step - loss: 0.0375 - acc: 0.9884 - val_loss: 0.5234 - val_acc: 0.9412\n",
      "Epoch 13/50\n",
      "102238/102238 [==============================] - 51s 502us/step - loss: 0.0356 - acc: 0.9888 - val_loss: 0.7452 - val_acc: 0.9438\n",
      "Epoch 14/50\n",
      "102238/102238 [==============================] - 51s 502us/step - loss: 0.0336 - acc: 0.9893 - val_loss: 0.4337 - val_acc: 0.9418\n",
      "Epoch 15/50\n",
      "102238/102238 [==============================] - 51s 501us/step - loss: 0.0314 - acc: 0.9901 - val_loss: 0.5978 - val_acc: 0.9400\n",
      "Epoch 16/50\n",
      "102238/102238 [==============================] - 52s 507us/step - loss: 0.0296 - acc: 0.9906 - val_loss: 0.6656 - val_acc: 0.9422\n",
      "Epoch 17/50\n",
      "102238/102238 [==============================] - 52s 512us/step - loss: 0.0292 - acc: 0.9905 - val_loss: 0.5172 - val_acc: 0.9346\n",
      "Epoch 18/50\n",
      "102238/102238 [==============================] - 52s 506us/step - loss: 0.0282 - acc: 0.9912 - val_loss: 0.5956 - val_acc: 0.9433\n",
      "Epoch 19/50\n",
      "102238/102238 [==============================] - 52s 506us/step - loss: 0.0276 - acc: 0.9914 - val_loss: 0.6469 - val_acc: 0.9418\n",
      "Epoch 20/50\n",
      "102238/102238 [==============================] - 51s 500us/step - loss: 0.0259 - acc: 0.9918 - val_loss: 0.7038 - val_acc: 0.9412\n",
      "Epoch 21/50\n",
      "102238/102238 [==============================] - 51s 503us/step - loss: 0.0251 - acc: 0.9918 - val_loss: 0.6598 - val_acc: 0.9394\n",
      "Epoch 22/50\n",
      "102238/102238 [==============================] - 51s 503us/step - loss: 0.0237 - acc: 0.9920 - val_loss: 0.7574 - val_acc: 0.9423\n",
      "Epoch 23/50\n",
      "102238/102238 [==============================] - 53s 514us/step - loss: 0.0240 - acc: 0.9924 - val_loss: 0.5982 - val_acc: 0.9370\n",
      "Epoch 24/50\n",
      "102238/102238 [==============================] - 52s 513us/step - loss: 0.0234 - acc: 0.9927 - val_loss: 0.7902 - val_acc: 0.9430\n",
      "Epoch 25/50\n",
      "102238/102238 [==============================] - 52s 512us/step - loss: 0.0223 - acc: 0.9931 - val_loss: 0.6339 - val_acc: 0.9397\n",
      "Epoch 26/50\n",
      "102238/102238 [==============================] - 52s 507us/step - loss: 0.0219 - acc: 0.9933 - val_loss: 0.7760 - val_acc: 0.9441\n",
      "Epoch 27/50\n",
      "102238/102238 [==============================] - 52s 509us/step - loss: 0.0216 - acc: 0.9931 - val_loss: 0.7525 - val_acc: 0.9436\n",
      "Epoch 28/50\n",
      "102238/102238 [==============================] - 52s 506us/step - loss: 0.0207 - acc: 0.9935 - val_loss: 0.6498 - val_acc: 0.9378\n",
      "Epoch 29/50\n",
      "102238/102238 [==============================] - 52s 505us/step - loss: 0.0196 - acc: 0.9937 - val_loss: 0.8058 - val_acc: 0.9422\n",
      "Epoch 30/50\n",
      "102238/102238 [==============================] - 52s 505us/step - loss: 0.0204 - acc: 0.9937 - val_loss: 0.8627 - val_acc: 0.9432\n",
      "Epoch 31/50\n",
      "102238/102238 [==============================] - 52s 506us/step - loss: 0.0205 - acc: 0.9932 - val_loss: 0.7745 - val_acc: 0.9422\n",
      "Epoch 32/50\n",
      "102238/102238 [==============================] - 52s 510us/step - loss: 0.0195 - acc: 0.9939 - val_loss: 0.8334 - val_acc: 0.9439\n",
      "Epoch 33/50\n",
      "102238/102238 [==============================] - 52s 511us/step - loss: 0.0183 - acc: 0.9940 - val_loss: 0.8269 - val_acc: 0.9425\n",
      "Epoch 34/50\n",
      "102238/102238 [==============================] - 53s 519us/step - loss: 0.0182 - acc: 0.9943 - val_loss: 1.1475 - val_acc: 0.9432\n",
      "Epoch 35/50\n",
      "102238/102238 [==============================] - 54s 527us/step - loss: 0.0185 - acc: 0.9941 - val_loss: 0.9382 - val_acc: 0.9429\n",
      "Epoch 36/50\n",
      "102238/102238 [==============================] - 53s 514us/step - loss: 0.0177 - acc: 0.9942 - val_loss: 0.9053 - val_acc: 0.9412\n",
      "Epoch 37/50\n",
      "102238/102238 [==============================] - 53s 514us/step - loss: 0.0177 - acc: 0.9942 - val_loss: 0.7824 - val_acc: 0.9384\n",
      "Epoch 38/50\n",
      "102238/102238 [==============================] - 51s 502us/step - loss: 0.0183 - acc: 0.9942 - val_loss: 0.8423 - val_acc: 0.9430\n",
      "Epoch 39/50\n",
      "102238/102238 [==============================] - 52s 509us/step - loss: 0.0177 - acc: 0.9945 - val_loss: 0.7817 - val_acc: 0.9378\n",
      "Epoch 40/50\n",
      "102238/102238 [==============================] - 52s 510us/step - loss: 0.0176 - acc: 0.9943 - val_loss: 0.8221 - val_acc: 0.9415\n",
      "Epoch 41/50\n",
      "102238/102238 [==============================] - 52s 511us/step - loss: 0.0173 - acc: 0.9946 - val_loss: 0.7575 - val_acc: 0.9393\n",
      "Epoch 42/50\n",
      "102238/102238 [==============================] - 52s 511us/step - loss: 0.0173 - acc: 0.9947 - val_loss: 0.9028 - val_acc: 0.9415\n",
      "Epoch 43/50\n",
      "102238/102238 [==============================] - 52s 509us/step - loss: 0.0164 - acc: 0.9947 - val_loss: 0.8865 - val_acc: 0.9403\n",
      "Epoch 44/50\n",
      "102238/102238 [==============================] - 52s 512us/step - loss: 0.0172 - acc: 0.9947 - val_loss: 0.7717 - val_acc: 0.9421\n",
      "Epoch 45/50\n",
      "102238/102238 [==============================] - 52s 512us/step - loss: 0.0161 - acc: 0.9950 - val_loss: 0.8100 - val_acc: 0.9410\n",
      "Epoch 46/50\n",
      "102238/102238 [==============================] - 52s 509us/step - loss: 0.0159 - acc: 0.9949 - val_loss: 1.0071 - val_acc: 0.9436\n",
      "Epoch 47/50\n",
      "102238/102238 [==============================] - 52s 512us/step - loss: 0.0169 - acc: 0.9947 - val_loss: 0.9682 - val_acc: 0.9432\n",
      "Epoch 48/50\n",
      "102238/102238 [==============================] - 53s 514us/step - loss: 0.0162 - acc: 0.9950 - val_loss: 0.9699 - val_acc: 0.9432\n",
      "Epoch 49/50\n",
      "102238/102238 [==============================] - 52s 513us/step - loss: 0.0157 - acc: 0.9951 - val_loss: 0.7405 - val_acc: 0.9391\n",
      "Epoch 50/50\n",
      "102238/102238 [==============================] - 52s 511us/step - loss: 0.0148 - acc: 0.9954 - val_loss: 0.9618 - val_acc: 0.9413\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "hehe = load_model('CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12623/12623 [==============================] - 2s 180us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.01      0.02     11938\n",
      "           1       0.05      0.88      0.09       685\n",
      "\n",
      "    accuracy                           0.06     12623\n",
      "   macro avg       0.31      0.44      0.06     12623\n",
      "weighted avg       0.55      0.06      0.02     12623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "y_pred_bool = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(classification_report(data_test['truth'], y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     11938\n",
      "           1       0.42      0.12      0.19       685\n",
      "\n",
      "    accuracy                           0.94     12623\n",
      "   macro avg       0.69      0.56      0.58     12623\n",
      "weighted avg       0.92      0.94      0.93     12623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = [1 if x==0 else 0 for x in y_pred_bool]\n",
    "print(classification_report(data_test['truth'], temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"TEXT\"].tolist())\n",
    "temprun = tokenizer.texts_to_sequences(['WHAT THE FUCK BROTHAAAA'])\n",
    "temprun = pad_sequences(temprun, maxlen=200)\n",
    "\n",
    "predictions = model.predict(temprun, batch_size=1024, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12484\n",
       "0      139\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.DataFrame(y_pred_bool)\n",
    "data_test['truth'].value_counts()\n",
    "#temp[0].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow1.1] *",
   "language": "python",
   "name": "conda-env-tensorflow1.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
