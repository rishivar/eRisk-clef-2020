{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import xml.etree.ElementTree as et\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject671</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject9917</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject6238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject7238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>subject2182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>subject9829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>subject3270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>subject6464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>subject8721</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  label\n",
       "0     subject671      1\n",
       "1    subject9917      1\n",
       "2    subject6238      1\n",
       "3    subject8581      1\n",
       "4    subject7238      1\n",
       "..           ...    ...\n",
       "335  subject2182      0\n",
       "336  subject9829      0\n",
       "337  subject3270      0\n",
       "338  subject6464      0\n",
       "339  subject8721      0\n",
       "\n",
       "[340 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train/golden_truth.txt', sep=\" \")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XML(xml_file, df_cols): \n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    \n",
    "    for node in xroot: \n",
    "        res = []\n",
    "        res.append(node.attrib.get(df_cols[0]))\n",
    "        for el in df_cols[1:]: \n",
    "            if node is not None and node.find(el) is not None:\n",
    "                res.append(node.find(el).text)\n",
    "            else: \n",
    "                res.append(None)\n",
    "        rows.append({df_cols[i]: res[i] \n",
    "                     for i, _ in enumerate(df_cols)})\n",
    "    \n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "        \n",
    "    return out_df\n",
    "\n",
    "train = pd.DataFrame()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    df = parse_XML('train/data/'+row['id']+'.xml', ['ID','DATE', 'TEXT'])\n",
    "    df['truth'] = row['label']\n",
    "    df['ID'] = row['id']\n",
    "    train = pd.concat([train, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True, thresh=3)\n",
    "train = train.replace(to_replace='None', value=np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = \" \".join([lemmatizer.lemmatize(i) for i in text])\n",
    "    return lem_text\n",
    "\n",
    "\n",
    "train['TEXT'] = train['TEXT'].apply(lambda x: remove_punctuation(x), 1)\n",
    "train['TEXT'] = train['TEXT'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "train['TEXT'] = train['TEXT'].apply(lambda x: remove_stopwords(x))\n",
    "train['TEXT'] = train['TEXT'].apply(lambda x: word_lemmatizer(x))\n",
    "train['TEXT'] = train['TEXT'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "train['TEXT'] = train['TEXT'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "train.dropna(inplace=True)\n",
    "del train['DATE']\n",
    "del train['ID']\n",
    "train.to_csv('train.csv', header = True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow1.1] *",
   "language": "python",
   "name": "conda-env-tensorflow1.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
